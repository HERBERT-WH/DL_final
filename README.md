# NeRF-pytorch

[NeRF](http://www.matthewtancik.com/nerf) (Neural Radiance Fields) is a method that achieves state-of-the-art results for synthesizing novel views of complex scenes. Here are some videos generated by this repository (pre-trained models are provided below):

![](https://user-images.githubusercontent.com/7057863/78472232-cf374a00-7769-11ea-8871-0bc710951839.gif)
![](https://user-images.githubusercontent.com/7057863/78472235-d1010d80-7769-11ea-9be9-51365180e063.gif)

This project is a faithful PyTorch implementation of [NeRF](http://www.matthewtancik.com/nerf) that **reproduces** the results while running **1.3 times faster**. The code is based on authors' Tensorflow implementation [here](https://github.com/bmild/nerf), and has been tested to match it numerically.

## 新增功能

- ✅ **测试集PSNR计算**: 训练过程中自动计算测试集PSNR并记录到TensorBoard
- ✅ **数据预处理工具**: 提供 `data.py`脚本对自定义图像进行下采样处理
- ✅ **完整的LLFF数据格式说明**: 详细说明如何准备自己的数据

## Installation

```
git clone https://github.com/yenchenlin/nerf-pytorch.git
cd nerf-pytorch
pip install -r requirements.txt
```

<details>
  <summary> Dependencies (click to expand) </summary>

## Dependencies

- PyTorch 1.4
- matplotlib
- numpy
- imageio
- imageio-ffmpeg
- configargparse
- tensorboard (for logging)
- opencv-python (for data preprocessing)

The LLFF data loader requires ImageMagick.

You will also need the [LLFF code](http://github.com/fyusion/llff) (and COLMAP) set up to compute poses if you want to run on your own real data.

</details>

## 准备自己的数据

### 第一步：图像下采样预处理

如果你有自己的图像数据，可以使用提供的 `data.py`脚本进行下采样处理：

1. **修改 `data.py`中的路径**：

```python
# 修改输入图像路径
dir_path = 'your/input/images/path'
input_path = "your/input/images/path"
output_path = "your/output/images/path"

# 设置下采样倍数
n = 8  # 8倍下采样，可选择4或8
```

2. **运行下采样脚本**：

```bash
python data.py
```

这将把原始图像下采样8倍（或4倍），显著减少训练时间。

### 第二步：LLFF数据格式准备

要使用自己的数据训练NeRF，需要准备符合LLFF格式的数据。以 `data/nerf_llff_data/input_earth`为例，完整的LLFF数据文件夹应包含：

```
input_earth/                    # 数据集根目录
├── images/                     # 图像文件夹
│   ├── input_0001.jpg         # 按序号命名的图像
│   ├── input_0002.jpg
│   ├── ...
│   └── input_0216.jpg
├── poses_bounds.npy           # 相机姿态和边界信息
├── sparse/                    # COLMAP重建结果
│   └── 0/
│       ├── cameras.bin        # 相机参数
│       ├── images.bin         # 图像信息
│       ├── points3D.bin       # 3D点云
│       └── project.ini        # 项目配置
└── database.db               # COLMAP数据库文件
```

#### 关键文件说明：

1. **images/**: 包含所有输入图像

   - 图像应按序号命名（如 `input_0001.jpg`）
   - 建议图像数量：20-100张
   - 图像应围绕目标物体拍摄，覆盖不同角度
2. **poses_bounds.npy**: 相机姿态矩阵

   - 由LLFF代码生成
   - 包含每个图像的相机外参和场景边界
3. **sparse/**: COLMAP Structure-from-Motion结果

   - `cameras.bin`: 相机内参（焦距、主点等）
   - `images.bin`: 图像姿态信息
   - `points3D.bin`: 稀疏3D重建点云
   - `project.ini`: COLMAP项目配置
4. **database.db**: COLMAP特征匹配数据库

#### 生成LLFF数据的步骤：

1. **安装COLMAP和LLFF**：

```bash
# 安装COLMAP
# Ubuntu: sudo apt install colmap
# Windows: 下载COLMAP二进制文件

# 克隆LLFF代码
git clone https://github.com/Fyusion/LLFF.git
cd LLFF
pip install -e .
```

2. **运行LLFF处理脚本**：

```bash
python imgs2poses.py your_images_folder
```

这将生成所需的 `poses_bounds.npy`文件和COLMAP重建结果。

## How To Run?

### Quick Start

Download data for two example datasets: `lego` and `fern`

```
bash download_example_data.sh
```

To train a low-res `lego` NeRF:

```
python run_nerf.py --config configs/lego.txt
```

After training for 100k iterations (~4 hours on a single 2080 Ti), you can find the following video at `logs/lego_test/lego_test_spiral_100000_rgb.mp4`.

![](https://user-images.githubusercontent.com/7057863/78473103-9353b300-7770-11ea-98ed-6ba2d877b62c.gif)

---

To train a low-res `fern` NeRF:

```
python run_nerf.py --config configs/fern.txt
```

After training for 200k iterations (~8 hours on a single 2080 Ti), you can find the following video at `logs/fern_test/fern_test_spiral_200000_rgb.mp4` and `logs/fern_test/fern_test_spiral_200000_disp.mp4`

![](https://user-images.githubusercontent.com/7057863/78473081-58ea1600-7770-11ea-92ce-2bbf6a3f9add.gif)

---

### 训练自己的数据

准备好LLFF格式数据后，创建配置文件：

```bash
# 复制现有配置文件
cp configs/fern.txt configs/your_scene.txt

# 修改配置文件中的数据路径
# datadir = ./data/nerf_llff_data/your_scene
# expname = your_scene_test
```

开始训练：

```bash
python run_nerf.py --config configs/your_scene.txt
```

### 监控训练过程

训练过程中会自动计算测试集PSNR并记录到TensorBoard：

```bash
# 启动TensorBoard查看训练过程
tensorboard --logdir ./logs/your_scene_test/tensorboard

# 在浏览器中打开 http://localhost:6006
```

TensorBoard中可以查看：

- **Loss/train**: 训练损失
- **PSNR/train**: 训练PSNR
- **PSNR/test**: 测试集PSNR（每50000步计算一次）
- **Learning_rate**: 学习率变化
- **Render/**: 渲染结果可视化

### More Datasets

To play with other scenes presented in the paper, download the data [here](https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1). Place the downloaded dataset according to the following directory structure:

```
├── configs                                                                                                   
│   ├── ...                                                                                 
│                                                                                           
├── data                                                                                                                                                                                                   
│   ├── nerf_llff_data                                                                                              
│   │   └── fern                                                                                                                         
│   │   └── flower  # downloaded llff dataset                                                                              
│   │   └── horns   # downloaded llff dataset
|   |   └── input_earth  # 自定义LLFF数据示例
|   |   └── ...
|   ├── nerf_synthetic
|   |   └── lego
|   |   └── ship    # downloaded synthetic dataset
|   |   └── ...
```

---

To train NeRF on different datasets:

```bash
python run_nerf.py --config configs/{DATASET}.txt
```

replace `{DATASET}` with `trex` | `horns` | `flower` | `fortress` | `lego` | etc.

---

To test NeRF trained on different datasets:

```bash
python run_nerf.py --config configs/{DATASET}.txt --render_only
```

replace `{DATASET}` with `trex` | `horns` | `flower` | `fortress` | `lego` | etc.

### Pre-trained Models

You can download the pre-trained models [here](https://drive.google.com/drive/folders/1jIr8dkvefrQmv737fFm2isiT6tqpbTbv). Place the downloaded directory in `./logs` in order to test it later. See the following directory structure for an example:

```
├── logs 
│   ├── fern_test
│   ├── flower_test  # downloaded logs
│   ├── trex_test    # downloaded logs
│   └── your_scene_test  # 自定义场景训练结果
│       ├── tensorboard/  # TensorBoard日志
│       ├── testset_050000/  # 测试集渲染结果
│       └── *.tar  # 模型检查点
```

### Reproducibility

Tests that ensure the results of all functions and training loop match the official implmentation are contained in a different branch `reproduce`. One can check it out and run the tests:

```bash
git checkout reproduce
py.test
```

## Method

[NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](http://tancik.com/nerf)
 [Ben Mildenhall](https://people.eecs.berkeley.edu/~bmild/)\*`<sup>`1 `</sup>`,
 [Pratul P. Srinivasan](https://people.eecs.berkeley.edu/~pratul/)\*`<sup>`1 `</sup>`,
 [Matthew Tancik](http://tancik.com/)\*`<sup>`1 `</sup>`,
 [Jonathan T. Barron](http://jonbarron.info/) `<sup>`2 `</sup>`,
 [Ravi Ramamoorthi](http://cseweb.ucsd.edu/~ravir/) `<sup>`3 `</sup>`,
 [Ren Ng](https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html) `<sup>`1 `</sup>` `<br>`
 `<sup>`1 `</sup>`UC Berkeley, `<sup>`2 `</sup>`Google Research, `<sup>`3 `</sup>`UC San Diego
  \*denotes equal contribution

<img src='imgs/pipeline.jpg'/>

> A neural radiance field is a simple fully connected network (weights are ~5MB) trained to reproduce input views of a single scene using a rendering loss. The network directly maps from spatial location and viewing direction (5D input) to color and opacity (4D output), acting as the "volume" so we can use volume rendering to differentiably render new views

## Citation

Kudos to the authors for their amazing results:

```
@misc{mildenhall2020nerf,
    title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
    author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
    year={2020},
    eprint={2003.08934},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
```

However, if you find this implementation or pre-trained models helpful, please consider to cite:

```
@misc{lin2020nerfpytorch,
  title={NeRF-pytorch},
  author={Yen-Chen, Lin},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished={\url{https://github.com/yenchenlin/nerf-pytorch/}},
  year={2020}
}
```
